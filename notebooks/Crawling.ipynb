{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import sys\n",
    "import os\n",
    "from time import sleep\n",
    "\n",
    "from httpx import TimeoutException\n",
    "\n",
    "from scr.helpers.write import *\n",
    "from scr.helpers.read import *\n",
    "from scr.helpers.crawl import *\n",
    "\n",
    "sys.path.append(\".\")\n",
    "# Parameters\n",
    "SITE_NAME = \"coop\"\n",
    "PATH_CSV = os.path.join(\"../data/raw\", \"csv\", SITE_NAME)\n",
    "\n",
    "\n",
    "# Define class\n",
    "class Coop:\n",
    "    def __init__(self, driver):\n",
    "        # Parameters\n",
    "        self.BROWSER = driver\n",
    "        self.BASE_URL = \"https://cooponline.vn\"\n",
    "        self.DATE = str(datetime.date.today())\n",
    "        self.OBSERVATION = 0\n",
    "        # Define wait\n",
    "        self.wait = WebDriverWait(self.BROWSER, 10)\n",
    "        # Scroll options\n",
    "        self.SCROLL_PAUSE_TIME = 5\n",
    "        # Classes\n",
    "        self.wr = CSV_write(\"coop\")\n",
    "\n",
    "    def choose_location(self):\n",
    "        \"\"\"Choose shopping location on cooponline.vn\"\"\"\n",
    "        # Access url\n",
    "        self.BROWSER.get(self.BASE_URL)\n",
    "        sleep(2)\n",
    "        # Choose mart\n",
    "        district = Select(self.BROWSER.find_element(By.XPATH, \"(//select)[2]\"))\n",
    "        district.select_by_index(1)  # 01\n",
    "        sleep(2)\n",
    "        ward = Select(self.BROWSER.find_element(By.XPATH, \"(//select)[3]\"))\n",
    "        ward.select_by_index(1)  # Ben Thanh\n",
    "        sleep(2)\n",
    "        mart = Select(self.BROWSER.find_element(By.XPATH, \"(//select)[4]\"))\n",
    "        mart.select_by_index(1)  # Tan Phong\n",
    "        sleep(2)\n",
    "        # Click button\n",
    "        self.BROWSER.find_element(By.XPATH, \"//button[contains(text(), 'Xác nhận')]\").click()\n",
    "        sleep(2)\n",
    "\n",
    "    def disable_sub(self):\n",
    "        \"\"\"Disable subscription popup\"\"\"\n",
    "        try:\n",
    "            self.wait.until(EC.presence_of_element_located((By.ID, \"slidedown-footer\")))\n",
    "            self.BROWSER.find_element(\n",
    "                By.XPATH, \"//button[contains(@id, 'onesignal-slidedown-cancel-button')]\"\n",
    "            ).click()\n",
    "        except TimeoutException:\n",
    "            pass\n",
    "\n",
    "    def get_category_list(self) -> list:\n",
    "        \"\"\"Get list of relative categories directories from the top page\"\"\"\n",
    "        # Access to browser\n",
    "        self.BROWSER.get(self.BASE_URL)\n",
    "        sleep(5)\n",
    "        # Get soup\n",
    "        toppage_soup = BeautifulSoup(self.BROWSER.page_source, features=\"lxml\")\n",
    "        # Get categories\n",
    "        categories_bar = toppage_soup.find(\"ul\", {\"class\": \"megamenu\"}).find_all(\"li\")\n",
    "        # Create an empty list to store categories' information\n",
    "        page_list = []\n",
    "        for cat in categories_bar:\n",
    "            cat_l1 = cat.find(\"a\").text.strip()\n",
    "            cat_l2s = cat.find_all(\"div\", class_=\"menu\")\n",
    "            for child_cat in cat_l2s:\n",
    "                cat_l2 = child_cat.find(\"a\", class_=\"main-menu\").text.strip()\n",
    "                cat_l3s = child_cat.find_all(\"a\", class_=None)\n",
    "                for grandchild_cat in cat_l3s:\n",
    "                    row = {}\n",
    "                    row[\"cat_l1\"] = cat_l1\n",
    "                    row[\"cat_l2\"] = cat_l2\n",
    "                    row[\"cat_l3\"] = grandchild_cat.text.strip()\n",
    "                    row[\"href\"] = grandchild_cat[\"href\"]\n",
    "                    page_list.append(row)\n",
    "        # Remove duplicates\n",
    "        page_list = [dict(t) for t in set(tuple(i.items()) for i in page_list)]\n",
    "        return page_list\n",
    "\n",
    "    def scrap_data(self, cat: dict):\n",
    "        \"\"\"Get item data from a category page and self.write to csv\"\"\"\n",
    "        # Access\n",
    "        self.BROWSER.get(cat[\"href\"])\n",
    "        # Get soup\n",
    "        soup = BeautifulSoup(self.BROWSER.page_source, \"lxml\")\n",
    "        # Click see_more button as many as possible\n",
    "        while True:\n",
    "            try:\n",
    "                # Wait\n",
    "                self.wait.until(\n",
    "                    EC.presence_of_element_located((By.XPATH, \"//span[text()=' Xem tiếp . . .']\"))\n",
    "                )\n",
    "                see_more = self.BROWSER.find_element(By.XPATH, \"//span[text()=' Xem tiếp . . .']\")\n",
    "                see_more.click()\n",
    "            except IGNORED_EXCEPTIONS:\n",
    "                print(\n",
    "                    \"Clicked all see_more button as much as possible in \"\n",
    "                    + cat[\"cat_l3\"]\n",
    "                    + \" category.\"\n",
    "                )\n",
    "                break\n",
    "        # Scraping product's data\n",
    "        # try:\n",
    "        soup = BeautifulSoup(self.BROWSER.page_source, features=\"lxml\")\n",
    "        sleep(10)\n",
    "        # Get all products' holders\n",
    "        list = soup.find_all(\"div\", {\"class\": \"product-item-container\"})\n",
    "        print(\"Found \" + str(len(list)) + \" products\")\n",
    "        # Scraping data\n",
    "        for item in list:\n",
    "            row = {}\n",
    "            row[\"cat_l1\"] = cat[\"cat_l1\"]\n",
    "            row[\"cat_l2\"] = cat[\"cat_l2\"]\n",
    "            row[\"cat_l3\"] = cat[\"cat_l3\"]\n",
    "            # Name\n",
    "            product_name = item.find(\"div\", class_=\"caption\").find(\"a\").text.strip()\n",
    "            row[\"product_name\"] = product_name\n",
    "            # Brand\n",
    "            href = item.find(\"a\")[\"href\"]\n",
    "            prod_res = requests.get(href)\n",
    "            prod_soup = BeautifulSoup(prod_res.content, features=\"lxml\")\n",
    "            try:\n",
    "                brand_holder = prod_soup.find(\"h4\").find(\"a\")\n",
    "                row[\"brand\"] = brand_holder.text.strip()\n",
    "            except AttributeError:\n",
    "                row[\"brand\"] = \"\"\n",
    "            row[\"href\"] = href\n",
    "            # # Price\n",
    "            if item.find(\"span\", class_=\"price-new col-xs-12\") != None:\n",
    "                price = item.find(\"span\", class_=\"price-new col-xs-12\").text.strip()\n",
    "                price = price.split(\"đ\")[0].strip()\n",
    "                price = price.replace(\",\", \"\")\n",
    "                row[\"price\"] = int(price)\n",
    "            else:\n",
    "                None\n",
    "            # Old price\n",
    "            if item.find(\"span\", class_=\"price-old\") != None:\n",
    "                old_price = item.find(\"span\", class_=\"price-old\").text.strip()\n",
    "                old_price = old_price.split(\"đ\")[0].strip()\n",
    "                old_price = old_price.replace(\",\", \"\")\n",
    "                row[\"old_price\"] = int(old_price)\n",
    "            else:\n",
    "                None\n",
    "            self.OBSERVATION += 1\n",
    "            self.wr.write_data(row)\n",
    "        print(\"Finished scraping \" + cat[\"cat_l3\"] + \" category.\")\n",
    "        # except Exception as e:\n",
    "        #     print(\"Error on \" + self.BROWSER.current_url)\n",
    "        #     print(type(e).__name__ + str(e))\n",
    "        #     pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clicked all see_more button as much as possible in Nui, mì, bún, bánh tráng category.\n",
      "Found 39 products\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "dict contains fields not in fieldnames: 'old_price'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 43\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;66;03m# # Bước 4: Duyệt qua từng danh mục và thu thập dữ liệu\u001b[39;00m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m category \u001b[38;5;129;01min\u001b[39;00m categories:\n\u001b[1;32m---> 43\u001b[0m     \u001b[43mcoop_scraper\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscrap_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcategory\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;66;03m# # Đóng trình duyệt khi hoàn tất\u001b[39;00m\n\u001b[0;32m     48\u001b[0m \n\u001b[0;32m     49\u001b[0m \n\u001b[0;32m     50\u001b[0m \n\u001b[0;32m     51\u001b[0m \u001b[38;5;66;03m# driver.quit()\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[22], line 155\u001b[0m, in \u001b[0;36mCoop.scrap_data\u001b[1;34m(self, cat)\u001b[0m\n\u001b[0;32m    153\u001b[0m         \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    154\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mOBSERVATION \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m--> 155\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrow\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    156\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFinished scraping \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m cat[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcat_l3\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m category.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\ACER\\Desktop\\Data Analysis\\Scrapping\\Scrapping-FMCG-Tool\\scr\\helpers\\write.py:95\u001b[0m, in \u001b[0;36mCSV_write.write_data\u001b[1;34m(self, item_data)\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m file_exists:\n\u001b[0;32m     94\u001b[0m     writer\u001b[38;5;241m.\u001b[39mwriteheader()\n\u001b[1;32m---> 95\u001b[0m \u001b[43mwriter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwriterow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mitem_data\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ACER\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\csv.py:154\u001b[0m, in \u001b[0;36mDictWriter.writerow\u001b[1;34m(self, rowdict)\u001b[0m\n\u001b[0;32m    153\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwriterow\u001b[39m(\u001b[38;5;28mself\u001b[39m, rowdict):\n\u001b[1;32m--> 154\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwriter\u001b[38;5;241m.\u001b[39mwriterow(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dict_to_list\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrowdict\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32mc:\\Users\\ACER\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\csv.py:149\u001b[0m, in \u001b[0;36mDictWriter._dict_to_list\u001b[1;34m(self, rowdict)\u001b[0m\n\u001b[0;32m    147\u001b[0m     wrong_fields \u001b[38;5;241m=\u001b[39m rowdict\u001b[38;5;241m.\u001b[39mkeys() \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfieldnames\n\u001b[0;32m    148\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m wrong_fields:\n\u001b[1;32m--> 149\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdict contains fields not in fieldnames: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    150\u001b[0m                          \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([\u001b[38;5;28mrepr\u001b[39m(x) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m wrong_fields]))\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (rowdict\u001b[38;5;241m.\u001b[39mget(key, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrestval) \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfieldnames)\n",
      "\u001b[1;31mValueError\u001b[0m: dict contains fields not in fieldnames: 'old_price'"
     ]
    }
   ],
   "source": [
    "service = ChromeService(ChromeDriverManager().install())\n",
    "driver = webdriver.Chrome(service=service)\n",
    "\n",
    "\n",
    "# Tạo đối tượng Coop\n",
    "\n",
    "\n",
    "coop_scraper = Coop(driver)\n",
    "\n",
    "\n",
    "\n",
    "# Bước 1: Chọn địa điểm mua hàng\n",
    "\n",
    "\n",
    "coop_scraper.choose_location()\n",
    "\n",
    "\n",
    "\n",
    "# Bước 2: Tắt popup nếu có\n",
    "\n",
    "\n",
    "coop_scraper.disable_sub()\n",
    "\n",
    "\n",
    "\n",
    "# Bước 3: Lấy danh sách danh mục sản phẩm\n",
    "\n",
    "\n",
    "categories = coop_scraper.get_category_list()\n",
    "\n",
    "\n",
    "\n",
    "# # Bước 4: Duyệt qua từng danh mục và thu thập dữ liệu\n",
    "\n",
    "\n",
    "for category in categories:\n",
    "\n",
    "    coop_scraper.scrap_data(category)\n",
    "\n",
    "\n",
    "\n",
    "# # Đóng trình duyệt khi hoàn tất\n",
    "\n",
    "\n",
    "# driver.quit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
